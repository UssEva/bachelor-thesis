% !TeX spellcheck = en_US
% !TeX encoding = UTF-8

% COMPILE WITH:
% `latexmk`
% You need lualatex and biber (in all TeXLive distributions)

\documentclass[
    numbers=noenddot,
    %listof=totoc,
    parskip=half-,
    fontsize=12pt,
    paper=a4,
    oneside,
    titlepage,
    bibliography=totoc,
    chapterprefix=false,
%    draft
]{scrbook}

%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{array}

\newcommand{\bugram}{\textsc{Bugram}}
\newcommand{\litterbox}{\textsc{LitterBox}}
\newcommand{\scratch}{\textsc{Scratch}}
\newcommand{\jadet}{\textsc{JADET}}
\newcommand{\java}{\textsc{Java}}
\newcommand{\hairball}{\textsc{Hairball}}
\newcommand{\drscratch}{\textsc{Dr. Scratch}}

%pgfplots
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize

% use lualatex or xelatex
%\usepackage{fontspec}
\usepackage[onehalfspacing]{setspace}

% better language support
\usepackage{polyglossia}
\setdefaultlanguage{english}
%\setdefaultlanguage{german}

\usepackage{tocbasic}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}

\usepackage[]{scrlayer-scrpage}

% better bibliography (biblatex style)
% use biber to compile
\usepackage[citestyle=alphabetic, bibstyle=alphabetic, sorting=nyt, backend=biber, language=english, backref=true, maxcitenames=2]{biblatex}

% better quotes
% use \enquote{text}
\usepackage[autostyle,english=american,german=quotes]{csquotes}
\addbibresource{bibliography.bib}

% appendix
\usepackage[titletoc]{appendix}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% where to put all images and figures
\graphicspath{{images/}}



% Title
\title{Identification of Bugs in Scratch Programs with N-gram Language Models}

% Author
\author{Eva Gründinger}

% Date
\date{\today}

% CHOOSE ACCORDINGLY
\newcommand{\thesisType}{Exposé}
%\newcommand{\thesisType}{Masterarbeit}

\newcommand{\thetitle}{\@title}
\newcommand{\theauthor}{\@author}
\newcommand{\thedate}{\@date}

\pagestyle{scrheadings}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \frontmatter
    \include{include/EP-titlepage}
    \tableofcontents
    \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \mainmatter

    \chapter{Problem}\label{ch:problem}
    In order to improve code quality and reliability of programs, many rule-based techniques have been researched to detect bugs and code smells. However, these rule-based approaches are relying on highly frequent patterns that can then be used to determine if a line of code is considered a bug. 
    
    In this bachelor thesis, a different technique that uses n-gram language models to automatically detect bugs in \scratch{} programs is proposed. This way of approaching the detection of defective code is already successfully applied by \bugram{} on \java{} code. Token sequences of programs are assessed by their probability in the learned model while low probability ones are marked as bugs. The assumption is that low probability token sequences are unusual, which may indicate bugs, bad practices or special uses of code. 
    
    For bug detection the four main components that need to be configurated for the model are: \textit{Gram Size, Sequence Length, Reporting Size and Minimun Token Occurence}. After adjusting these settings, the n-gram Markov model is able to obtain the probabilities of all token sequences. Token sequences in \scratch{} consist of blocks that can be arranged by the user with a drag-and-drop principle. The probability of each block in a sequence is only determined by its previous n-1 tokens. Using a 3-gram model the probability of the sequence s is:
    \(P(s) = P(b_{1})P(b_{2}\mid b_{1})P(b_{3}\mid b_{1}b_{2})P(b_{4}\mid b_{2}b_{3}) \). Then the language model ranks the outcome based on the probabilities in descending order and reports the sequences with the lowest probabilities as potential bugs. 
    

    \chapter{State of the art}\label{ch:state-of-the-art}
    
    
    \section{Analysing Scratch programs}\label{sec:analyzing-scratch}

    \section{N-gram language models}\label{sec:language-models}
    Wang et al. demonstrated with their tool \bugram{} that n-gram language models can be used to find defective source code. Although there are other studies that covered the usage of n-grams for detecting clone bugs, localizing faults and code search, these did not leverage n-gram models. In contrast to n-grams that were used in these researches and are only token sequences, n-gram models are Markov models built on n-grams. The n-gram language model has also been widely used in modeling natural language and solving problems such as speech recognition or statistical machine translation.
    

    \section{Automatic bug detection}\label{sec:bugram}
    

    \chapter{Research questions}\label{ch:research-questions}
    The thesis aims to answer the following research questions:

    \begin{description}
        \item[RQ 1] What is the optimal gram size of the language model?
        \item[RQ 2] How long should the sequences be?
        \item[RQ 3] What reporting size is reasonable?
        \item[RQ 4] How many bugs are found?
        \item[RQ 5] What is the severity of violations?
    \end{description}


    \chapter{Evaluation}\label{ch:evaluation}
    
    Implement a visitor for n-gram language models in \litterbox{}. Use three representative \scratch{} projects to study the impact of different parameters on the performance.  
    
    \begin{description}
        \item[RQ 1 What is the optimal gram size of the language model?] 
        N-gram models for each project are built with the gram size ranging from two to ten. Calculate the probabilities of all token sequences and rank them based on their probabilities in descending order. Then examine the bottom 10, 20, 30, 50 and 100 sequences from each n-gram model and manually verify whether a token sequence contains a bug or not.
        \item[RQ 2 How long should the sequences be?]
        Use sequence lengths ranging from two to ten. For each built a n-gram model with the optimal gram size from [RQ1]. Calculate probabilities from all sequences.
        \item[RQ 3 What reporting size is reasonable?]
        For each examined project, built n-gram models with sequence lengths based on the results of [RQ1] and [RQ2]. Examine probabilities of all sequences and normalize the outcome. Narrow down the reporting size by only looking at the bottom 100 sequences. For each project, examine how many true bugs are detected when reporting size is equal to 10, 20, 30, 50 and 100.
    \end{description}

	Take another small set of \scratch{3} projects from our database which results can be manually verified.
	
    \begin{description}
        \item[RQ 4 How many bugs are found?] 
        Count all reported sequences. To reduce the candidate bug set, only token sequences at the bottom of at least two ranked lists generated by n-gram models with different sequence lengths are considered true bugs.
        \item[RQ 5 What is the severity of violations?] 
        Rank found violations, search for the actual \scratch{} projects and download them with their assets. Then classify them manually in the following categories: \textit{True Bugs, Refactoring Opportunities and False Positives}.
    \end{description}
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \backmatter

% -- Bibliography
    \printbibliography

\end{document}