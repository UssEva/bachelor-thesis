% !TeX spellcheck = en_US
% !TeX encoding = UTF-8

% COMPILE WITH:
% `latexmk`
% You need lualatex and biber (in all TeXLive distributions)

\documentclass[
    numbers=noenddot,
    %listof=totoc,
    parskip=half-,
    fontsize=12pt,
    paper=a4,
    oneside,
    titlepage,
    bibliography=totoc,
    chapterprefix=false,
%    draft
]{scrbook}

%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{array}

\newcommand{\latex}{\textsc{LaTex}}
\newcommand{\ngram}{\textsc{n-gram language model}}
\newcommand{\bugram}{\textsc{Bugram}}
\newcommand{\litterbox}{\textsc{LitterBox}}
\newcommand{\scratch}{\textsc{Scratch}}
\newcommand{\java}{\textsc{Java}}
\newcommand{\hairball}{\textsc{Hairball}}
\newcommand{\drscratch}{\textsc{Dr. Scratch}}
\newcommand{\whisker}{\textsc{Whisker}}

%pgfplots
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize

% use lualatex or xelatex
%\usepackage{fontspec}
\usepackage[onehalfspacing]{setspace}

% better language support
\usepackage{polyglossia}
\setdefaultlanguage{english}
%\setdefaultlanguage{german}

\usepackage{tocbasic}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}

\usepackage[]{scrlayer-scrpage}

% better bibliography (biblatex style)
% use biber to compile
\usepackage[citestyle=alphabetic, bibstyle=alphabetic, sorting=nyt, backend=biber, language=english, backref=true, maxcitenames=2]{biblatex}

% better quotes
% use \enquote{text}
\usepackage[autostyle,english=american,german=quotes]{csquotes}
\addbibresource{bibliography.bib}

% appendix
\usepackage[titletoc]{appendix}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% where to put all images and figures
\graphicspath{{images/}}



% Title
\title{Scratch Bug Detection Using the N-gram Language Model}

% Author
\author{Eva Gründinger}

% Date
\date{\today}

% CHOOSE ACCORDINGLY
\newcommand{\thesisType}{Exposé}
%\newcommand{\thesisType}{Masterarbeit}

\newcommand{\thetitle}{\@title}
\newcommand{\theauthor}{\@author}
\newcommand{\thedate}{\@date}

\pagestyle{scrheadings}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \frontmatter
    \include{include/EP-titlepage}
    \tableofcontents
    \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \mainmatter

    \chapter{Problem}\label{ch:problem}
    In order to improve code quality and reliability of programs, many rule-based techniques have been researched to detect bugs and code smells. However, these rule-based approaches are relying on highly frequent patterns in the code. 
    
    In this bachelor thesis a different technique that uses \ngram{s} to automatically detect bugs in \scratch{} programs is proposed. This way of approaching the detection of defective code is already successfully applied by \bugram{}~\cite{bugram} on \java{} code. Token sequences of programs are assessed by their probability in the learned model while low probability ones are marked as bugs. The assumption is that low probability token sequences are unusual, which may indicate bugs, bad practices or special uses of code. 
    
    For bug detection the four main components that need to be configured for the model are: \textit{Gram Size, Sequence Length, Reporting Size and Minimum Token Occurrence}. After adjusting these settings, the n-gram Markov model is able to obtain the probabilities of all token sequences. Token sequences in \scratch{} consist of blocks that can be arranged by the user with a drag-and-drop principle. The probability of each block in a sequence is only determined by its previous n-1 tokens. Using a 3-gram model the probability of the sequence s is:
    \(P(s) = P(b_{1})P(b_{2}\mid b_{1})P(b_{3}\mid b_{1}b_{2})P(b_{4}\mid b_{2}b_{3}) \). Then the language model ranks the outcome based on the probabilities in descending order and reports the sequences with the lowest probabilities as potential bugs. 
    

    \chapter{State of the art}\label{ch:state-of-the-art}
    
    \section{Analyzing Scratch programs}\label{sec:analyzing-scratch}
    The tools Dr. Scratch~\cite{drscratch} as well as Hairball~\cite{hairball} analyze \scratch{} programs to find bugs and code smells. These are then reported to the user in order to improve the computational thinking and coding skills of novice programmers. Bad programming habits were assessed in a preliminary study by Moreno et al.~\cite{badhabits} and code smells that are very common in Scratch were analyzed by Vargas-Alba et al.~\cite{badsmells}. Stahlbauer et al.~\cite{whisker} introduced \whisker{} which is a formal testing framework for Scratch. \litterbox, a tool created by Frädrich et al.~\cite{scratch_bugpatterns} that creates an AST of \scratch{} programs, is used for finding code smells as well as bug patterns. In this bachelor thesis, the goal is to enhance \litterbox{} by finding bugs with a n-gram language model instead of bug patterns and rules like all tools mentioned above are already able to do.
    

    \section{N-gram language models}\label{sec:language-models}
    Hindle et al.~\cite{naturalness} ﬁrst introduced the \ngram{} to show that software source code is highly repetitive and the \ngram{} can be used in code suggestion and completion. This work is the basis for using language models to model source code and demonstrated how they could be used in software tools. A very accurate algorithm by using a Hidden Markov Model for code completion was proposed by Han et al.~\cite{codecompletion}. SLAMC by Nguyen et al.~\cite{SLAMC}, which incorporated semantic information into a n-gram model, presented a method to code suggestion. It demonstrated how tokens can be seen more semantically instead of just syntactically. Raychev et al.~\cite{SLANG} investigated the eﬀectiveness of various language models for code completion, i.e., n-gram and recurrent neural networks. By combining program analysis and the n-gram model, they proposed SLANG, which had the goal to predict the sequence of method calls in a software system. This work leverages the \ngram{} on a new domain - software defect detection in \scratch{}.
    

    \section{Automatic bug detection}\label{sec:bugram}
    Wang et al.~\cite{bugram} demonstrated with their tool \bugram{} that \ngram{s} can be used to find defective code in \java{} programs. Although there are other studies that covered the usage of n-grams for detecting clone bugs~\cite{clonebugs}, localizing faults~\cite{faults} and code search~\cite{codesearch}, these did not leverage n-gram models. In contrast to n-grams that are only token sequences and were primarily used in the papers mentioned above, n-gram models are Markov models built on n-grams. The bachelor thesis will try to adapt this mechanism for Scratch projects.

    \chapter{Research questions}\label{ch:research-questions}
    The thesis aims to answer the following research questions:

    \begin{description}
        \item[RQ 1] What is the optimal gram size of the language model?
        \item[RQ 2] How long should the sequences be?
        \item[RQ 3] What reporting size is reasonable?
        \item[RQ 4] How many bugs are found?
        \item[RQ 5] What is the severity of violations?
    \end{description}


    \chapter{Evaluation}\label{ch:evaluation}
    
    Implement a visitor for \ngram{s} in \litterbox{}. Use three representative \scratch{} projects to study the impact of different parameters on the performance.  
    
    \begin{description}
        \item[RQ 1 What is the optimal gram size of the language model?] N-gram models for each project are built with the gram size ranging from two to ten. Calculate the probabilities of all token sequences and rank them based on their probabilities in descending order. Then examine the bottom 10, 20, 30, 50 and 100 sequences from each n-gram model and manually verify whether a token sequence contains a bug or not.
        \item[RQ 2 How long should the sequences be?]
        Use sequence lengths ranging from two to ten. For each built a n-gram model with the optimal gram size from [RQ 1]. Calculate probabilities from all sequences.
        \item[RQ 3 What reporting size is reasonable?]
        For each examined project, built n-gram models with sequence lengths based on the results of [RQ 1] and [RQ 2]. Examine probabilities of all sequences and normalize the outcome. Narrow down the reporting size by only looking at the bottom 100 sequences. For each project, examine how many true bugs are detected when reporting size is equal to 10, 20, 30, 50 and 100.
    \end{description}

	Take another small set of \scratch{3} projects from our database that can be manually verified.
	
    \begin{description}
        \item[RQ 4 How many bugs are found?] 
        Count all reported sequences. To reduce the candidate bug set, only token sequences at the bottom of at least two ranked lists generated by n-gram models with different sequence lengths are considered true bugs.
        \item[RQ 5 What is the severity of violations?] 
        Rank found violations, search for the actual \scratch{} projects and download them with their assets. Then classify them manually in the following categories: \textit{True Bugs, Refactoring Opportunities and False Positives}.
    \end{description}
 
 
 	\chapter{Schedule}\label{ch:schedule}
 	
 	\begin{table}[ht]
 		\begin{tabular}{p{1cm}p{3cm}p{3cm}p{8cm}}
 			Weeks & Timeslot & Phase & Tasks \\
 			\hline
 			1 & 23.03. - 29.03. & Preparation & Setup \latex{} outline, \litterbox{} branch, Search papers \\
 			3 & 30.03. - 19.04. & Programming & Build \ngram{} tool for \scratch{}: Tokenization, Calculate probabilities for sequences, Rank results and report bugs\\
 			3 & 20.04. - 10.05. & Evaluation & Find optimal parameters for model, Analyze reported bugs\\
 			3 & 11.05. - 31.05. & Writing & Structure, Research Questions, Main body, Introduction, Conclusion \\
 			1.5 & 01.06. - 10.06. & Correction & Bibliography, Content, Grammar \\
 			0.5 & 11.06. - 14.06. & Submission & Print and bind Bachelor thesis\\
 			\hline
 		\end{tabular}
 	\label{tab:schedule}
 	\caption{Schedule for Bachelor thesis}
 	\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \backmatter

% -- Bibliography
    \printbibliography
    
\end{document}