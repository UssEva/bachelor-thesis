% !TeX spellcheck = en_GB
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\newcommand{\numlarge}{152,007}
\newcommand{\monthstart}{December 2019}
\newcommand{\monthend}{January 2020}
\newcommand{\parsingexcp}{114}
\newcommand{\successfullyanalysed}{151,893}
\newcommand{\foundbugs}{100}
\newcommand{\modelduration}{72 h}
\newcommand{\bugfindingduration}{5 min}

\chapter{Evaluation}\label{chap:evaluation}
%TODO Unfinished chapter

The main research points for this bachelor's thesis are answers to the following questions:
\begin{itemize}
\item[\textbf{RQ1}] What is the optimal gram size for building an useful model?
\item[\textbf{RQ2}] How long should the analysed sequences be for an effective analysis process?
\item[\textbf{RQ3}] How effective are \ngram{s} for bug detection in comparison to \litterbox{}?
\item[\textbf{RQ4}] What kinds of violations were detected by the \ngram{}?
\end{itemize}
For the analysis a big dataset of \scratch{3} projects was used to create a descriptive model. Then a new set of projects was analysed to detect bugs and manually classify the problematic code blocks.


\section{Datasets}\label{sec:dataset}
This section shows the two different datasets that were used for the bachelor's thesis, specifically, for the training of the model like it is introduced in Subsection~\ref{subsec:trainingset} and for bug detection which Subsection~\ref{subsec:bugset} focuses on.

\subsection{Model Training Set}\label{subsec:trainingset}
%TODO Add evaluation time, amount, gadgets,...
In order to have a sufficient number of scripts and sprites to calculate a probability distribution from, we decided to build the \ngram{} on a large dataset. The dataset consists of \numlarge\ \scratch\ projects. From \monthstart\ to \monthend\ we downloaded the most recent projects with the \scratch\ REST API\footnote{\url{https://github.com/LLK/scratch-rest-api/wiki}, last accessed May 8, 2020}. We did not exclude remixes from the dataset.

\subsection{Bug Finding Set}\label{subsec:bugset}
The dataset for bug detection was downloaded from \scratch{} in August and randomly chosen in order to have an unbiased analysis of different types of \scratch{} projects. But the chosen projects had to fulfil a few criteria and restrictions to proof their suitability and make a manual examination possible. The project has to...
\begin{itemize}
\item ...not be empty.
\item ...have a minimum of 1 non-empty sprite.
\item ...have a maximum of 20 sprites.
\item ...have at least 2 code smells.
\item ...consist of 10 to 1000 blocks.
\end{itemize}

%TODO Update table
The following projects were chosen to test bug detection with the n-gram model, thus they fulfil all above mentioned criteria. 
\begin{table}[H]
    \centering
    \caption[Projects evaluated in our experiments]{\label{tab:projects}Projects evaluated in our experiments}
    \begin{tabular}{lrrr}
        \toprule
        Project & Size & \#Sprites & \#Blocks \\
        \midrule
        Fruit Catching & kb & 42 & 295 \\
        \bottomrule
    \end{tabular}
\end{table}


\section{RQ1: Gram Size}\label{sec:gram_size}
%TODO Update execution
In order to find the best \textit{gram size} that should be used for the model building process, different n-gram models with \textit{gram sizes} ranging from 2 to 10 were built. After calculating the probabilities of each token sequence and ranking them based on their probabilities in descending order, the bottom 10 sequences of each list were manually examined. The \textit{gram size} that managed to find the most true bugs was chosen. (Add result...)

%TODO Add diagram: Bugs per gram size.


\section{RQ2: Sequence Length}\label{sec:sequence_length}
%TODO Update execution
The evaluation of the optimal sequence length for the analysis was executed by building n-gram models with the \textit{gram size} from RQ \ref{sec:gram_size} and {sequence lengths} in the range from 2 to 10. Calculating probabilities from all sequences and ranking them was followed by examining the bottom 10 sequences with low probabilities to check how many true bugs are detected. The \textit{sequence length} that helped to find the most bugs, is chosen as the optimal number. (Add results...)

%TODO Add diagram: Bugs per sequence length


\section{RQ3: Comparison to Litterbox}\label{sec:litterbox}
%TODO Find bugs with litterbox
On the same analysis set, \litterbox{} was used to find bugs and count how many of these bugs the \ngram{} can detect as well. (Add results...)

%TODO Update table: Project #LitterboxBugs #NgramBugs
\begin{table}[H]
    \centering
    \caption[The number of reported bugs found by \litterbox{} and N-gram model]{\label{tab:litterbox}The number of reported bugs found by \litterbox{} and N-gram model}
    \begin{tabular}{lrr}
        \toprule
        Project & \#LitterboxBugs & \#NgramBugs \\
        \midrule
        Fruit Catching & 42 & 295 \\
        \bottomrule
    \end{tabular}
\end{table}
 
 
\section{RQ4: Violation Classification}\label{sec:violations}
%TODO Find bugs and show examples
Procedure  continued by analysing if \ngram{} found bugs that are not detected by \litterbox{}. After all potential bugs were collected in the candidate bug set, the defective code was manually classified into these categories: \textit{True Bugs, Refactoring Opportunities or False Positives}. (Add results...)

%TODO Update table: #ReportedBugs #TrueBugs #Refactoring #FalsePositive
\begin{table}[H]
    \centering
    \caption[The categorization of all reported bugs]{\label{tab:violations}The categorization of all reported bugs}
    \begin{tabular}{lrrr}
        \toprule
        \#ReportedBugs & \#TrueBugs & \#Refactoring & \#FalsePositive \\
        \midrule
        150 & 42 & 95 & 13 \\
        \bottomrule
    \end{tabular}
\end{table}


\section{Threats to Validity}\label{sec:threats-to-validity}
\paragraph{Implementation of N-gram Model.}
To compare the results of \litterbox{} with the n-gram model approach, we implemented the n-gram language model as close as possible to the information given from the \bugram{}~\cite{bugram} paper and based on the \AST{} that is created by \litterbox{} out of \scratch{} projects in JSON format. For our implementation, we have tried our best to tune the configuration parameters to obtain the best results. Our comparison is fair because both \litterbox{} as well as \ngram{} are evaluated on the same projects.

\paragraph{Bugs are manually verified.}
Following the \bugram{}~\cite{bugram} paper, reported bugs were assessed manually in order to approve and classify all low probability token sequences and distinguish between, i.e., true bugs, refactoring opportunities and false positives. Since we are not the original programmers of the analysed projects that are featured in this bachelor's thesis, the examination of the code is not objective. But because of common practice, it is an acceptable approach for the assessment of the given code.

