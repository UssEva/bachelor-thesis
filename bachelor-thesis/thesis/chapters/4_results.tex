% !TeX spellcheck = en_GB
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\newcommand{\numlarge}{152,007}
\newcommand{\monthstart}{December 2019}
\newcommand{\monthend}{January 2020}
\newcommand{\parsingexcp}{114}
\newcommand{\successfullyanalysed}{151,893}
\newcommand{\foundbugs}{100}
\newcommand{\modelduration}{72 h}
\newcommand{\bugfindingduration}{5 min}

\chapter{Evaluation}\label{chap:evaluation}
%TODO Unfinished chapter

The main research points for this bachelor's thesis are answers to the following questions:
\begin{itemize}
\item[\textbf{RQ1}] What is the optimal gram size for building an useful model?
\item[\textbf{RQ2}] How long should the analysed sequences be for an effective analysis process?
\item[\textbf{RQ3}] How effective are \ngram{s} for bug detection in comparison to \litterbox{}?
\item[\textbf{RQ4}] What kinds of violations were detected by the \ngram{}?
\end{itemize}


\section{Datasets}\label{sec:dataset}
This section shows the two different datasets that were used for the bachelor's thesis, specifically, for the training of the model like it is introduced in Subsection~\ref{subsec:trainingset} and for bug detection which Subsection~\ref{subsec:bugset} focuses on.

\subsection{Model Training Set}\label{subsec:trainingset}
%TODO Add evaluation time, amount, gadgets,...
In order to have a sufficient number of scripts and sprites to calculate a probability distribution from, we decided to build the \ngram{} on a large dataset. The dataset consists of \numlarge\ \scratch\ projects. From \monthstart\ to \monthend\ we downloaded the most recent projects with the \scratch\ REST API\footnote{\url{https://github.com/LLK/scratch-rest-api/wiki}, last accessed May 8, 2020}. We did not exclude remixes from the dataset.

\subsection{Bug Finding Set}\label{subsec:bugset}
%TODO Add used dataset with criteria, gadgets, time,...
The second dataset we use to evaluate the \ngram{} is a set of solutions for five small coding tasks for students. Based on the task the pupils had to implement, we call these sets \textit{Monkey}, \textit{Elephant}, \textit{Cat}, \textit{Horse} and \textit{Fruit Catching} task.
We used solutions of pupils which originate in primary programming education~\cite{katharina} for the Monkey, Elephant, Cat and Horse tasks. For the Fruit Catching task we used the same dataset as Stahlbauer et al. in their work about testing \scratch\ programs automatically~\cite{whisker}. Table~\ref{tab:big-dataset} shows the number of projects for each task. 
 
\begin{table}[H]
    \centering
    \caption[Projects evaluated in our experiments]{\label{tab:big-dataset}Projects evaluated in our experiments}
    \begin{tabular}{lr}
        \toprule
        Task & \#Projects \\
        \midrule
        Fruit Catching & 42 \\
        Monkey & 120 \\
        Elephant & 130 \\
        Cat & 129 \\
        Horse & 73 \\
        \bottomrule
    \end{tabular}
\end{table}


\section{RQ1: Gram Size}\label{sec:gram_size}
%TODO Update execution
In order to find the best \textit{gram size} that should be used for the model building process, different n-gram models with \textit{gram sizes} ranging from 2 to 10 were built. After calculating the probabilities of each token sequence and ranking them based on their probabilities in descending order, the bottom 10 sequences of each list were manually examined. The \textit{gram size} that managed to find the most true bugs was chosen. (Add result...)

%TODO Add diagram: Bugs per gram size.


\section{RQ2: Sequence Length}\label{sec:sequence_length}
%TODO Update execution
The evaluation of the optimal sequence length for the analysis was executed by building n-gram models with the \textit{gram size} from Section~\ref{sec:gram_size} and {sequence lengths} in the range from 2 to 10. Calculating probabilities from all sequences and ranking them was followed by examining the bottom 10 sequences with low probabilities to check how many true bugs are detected. The \textit{sequence length} that helped to find the most bugs, is chosen as the optimal number. (Add results...)

%TODO Add diagram: Bugs per sequence length


\section{RQ3: Comparison to Litterbox}\label{sec:litterbox}
%TODO Find bugs with litterbox, fix gram size and sequence lengh
After setting the \textit{gram size} and \textit{sequence length} both on a fixed size which the information that was gathered by the first research questions showed, the same set of projects is used for a comparison between \litterbox{} and \ngram{} approach. The projects get all analysed by \litterbox{} and the \ngram{} to test how many sequences are reported with the different methods. (Add results...)

%TODO Update table: Project #LitterboxBugs #NgramBugs
\begin{table}[H]
    \centering
    \caption[The number of reported bugs found by \litterbox{} and N-gram model]{\label{tab:litterbox}The number of reported bugs found by \litterbox{} and N-gram model}
    \begin{tabular}{lrr}
        \toprule
        Task & \#LitterboxBugs & \#NgramBugs \\
        \midrule
        Fruit Catching &  &\\
        Monkey &  &  \\
        Elephant &  & \\
        Cat &  &  \\
        Horse & &  \\
        \bottomrule
    \end{tabular}
\end{table}
 
 
\section{RQ4: Violation Classification}\label{sec:violations}
%TODO Find bugs and show examples
The analysis procedure is continued by analysing specifically the bugs that are reported by the \ngram{}. For each task the project with the most found bugs is manually analysed to estimate the rate of false positives. Table~\ref{tab:small-dataset} shows the projects that were chosen for further assessment as well as their important information.

\begin{table}[H]
    \centering
    \caption[Small Dataset with one project of each task]{\label{tab:small-dataset}Small dataset}
    \begin{tabular}{lrrrr}
        \toprule
        Task & Project & Size & \#Scripts & \#Blocks \\
        \midrule
        Fruit Catching  & & & & \\
        Monkey  & & & &  \\
        Elephant  & & & & \\
        Cat  & & & &  \\
        Horse & & & & \\
        \bottomrule
    \end{tabular}
\end{table}

After all by \ngram{} detected potential bugs were collected in the candidate bug set, the defective code is manually classified into the following categories: \textit{True Bugs, Refactoring Opportunities or False Positives}. Table~\ref{tab:violations} displays the numbers for each category.

%TODO Update table: #ReportedBugs #TrueBugs #Refactoring #FalsePositive
\begin{table}[H]
    \centering
    \caption[The categorization of all reported bugs]{\label{tab:violations}The categorization of all reported bugs}
    \begin{tabular}{lrrrr}
        \toprule
        Task & \#ReportedBugs & \#TrueBugs & \#Refactoring & \#FalsePositive \\
        \midrule
        Fruit Catching  & & & & \\
        Monkey  & & & &  \\
        Elephant  & & & & \\
        Cat  & & & &  \\
        Horse & & & & \\
        \bottomrule
    \end{tabular}
\end{table}


\section{Threats to Validity}\label{sec:threats-to-validity}
\paragraph{Implementation of N-gram Model.}
To compare the results of \litterbox{} with the n-gram model approach, we implemented the n-gram language model as close as possible to the information given from the \bugram{}~\cite{bugram} paper and based on the \AST{} that is created by \litterbox{} out of \scratch{} projects in JSON format. For our implementation, we have tried our best to tune the configuration parameters to obtain the best results. Our comparison is fair because both \litterbox{} as well as \ngram{} are evaluated on the same projects.

\paragraph{Bugs are manually verified.}
Following the \bugram{}~\cite{bugram} paper, reported bugs were assessed manually in order to approve and classify all low probability token sequences and distinguish between, i.e., true bugs, refactoring opportunities and false positives. Since we are not the original programmers of the analysed projects that are featured in this bachelor's thesis, the examination of the code is not objective. But because of common practice, it is an acceptable approach for the assessment of the given code.

