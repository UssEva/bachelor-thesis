% !TeX spellcheck = en_GB
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex
\chapter{Discussion}\label{chap:discussion}

\section{Examples}
\subsection{Example Bug}
Some of the detected actual bugs are shown in Figure \ref{fig:bugs}.

\subsection{False Positive}
In Figure \ref{fig:false_positives} there are some token sequences that were falsely reported as bugs but are false positives.

\subsection{Refactoring Opportunity}
When you analyse the blocks in Figure \ref{fig:refactoring}, you notice the refactoring opportunities that could make the code more easy to read and understand. But the actual functionality of these code blocks is working just fine and does not need to be fixed.


\section{Execution Time and Space}


\section{Related Work}
\subsection{Analysing Scratch programs}\label{sec:analyzing-scratch}
The tools \drscratch{}~\cite{drscratch} as well as \hairball{}~\cite{hairball} analyse \scratch{} programs to find bugs and code smells. These are then reported to the user in order to improve the computational thinking and coding skills of novice programmers. Bad programming habits were assessed in a preliminary study by Moreno et al.~\cite{badhabits} and code smells that are very common in Scratch were analysed by Vargas-Alba et al.~\cite{badsmells}. Stahlbauer et al.~\cite{whisker} introduced \whisker{} which is a formal testing framework for Scratch. \litterbox, a tool created by Fr√§drich et al.~\cite{scratch_bugpatterns} that creates an AST of \scratch{} programs, is used for finding code smells as well as bug patterns.

\subsection{Object Usage Anomalies}
Interactions with objects are required to follow a specific procedure, for example by a sequence of method calls. But these standards are rarely documented and can lead to problematic behaviour in the code. Wasylkowski et al.~\cite{object_usage} infers legal sequences of method calls to code examples. The results can then be used to find anomalies in the analyzed implementation. As a automatic defect detection algorithm, it is the first of its kind that uses method call sequences to learn and detect anomalies.

\subsection{N-gram language models}\label{sec:language-models}
Hindle et al.~\cite{naturalness} first introduced the \ngram{} to show that software source code is highly repetitive and the \ngram{} can be used in code suggestion and completion. This work is the basis for using language models to model source code and demonstrated how they could be used in software tools. A very accurate algorithm by using a Hidden Markov Model for code completion was proposed by Han et al.~\cite{codecompletion}. SLAMC by Nguyen et al.~\cite{SLAMC}, which incorporated semantic information into a n-gram model, presented a method to code suggestion. It demonstrated how tokens can be seen more semantically instead of just syntactically. Raychev et al.~\cite{SLANG} investigated the effectiveness of various language models for code completion, i.e., n-gram and recurrent neural networks. By combining program analysis and the n-gram model, they proposed SLANG, which had the goal to predict the sequence of method calls in a software system. 

\subsection{Bugram}
The effective usage of n-gram language models in the field of bug detection is also demonstrated by Wang et al.~\cite{bugram} with their tool \bugram{} that finds defective code with \ngram{s} in \java{} programs. Although there are other studies that covered the usage of n-grams for detecting clone bugs~\cite{clonebugs}, localizing faults~\cite{faults} and code search~\cite{codesearch}, these did not leverage n-gram models. In contrast to n-grams that are only token sequences, n-gram models are Markov models built on n-grams.