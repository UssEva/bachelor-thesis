% !TeX spellcheck = en_GB
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex
\chapter{Introduction}\label{chap:introduction}

In order to improve code quality and reliability of programs, many rule-based techniques have been researched to detect bugs and code smells. However, these rule-based approaches are relying on highly frequent patterns in the code. 

In this bachelor thesis a different technique that uses \ngram{s} to automatically detect bugs in \scratch{} programs is proposed. This way of approaching the detection of defective code is already successfully applied by \bugram{}~\cite{bugram} on \java{} code. Token sequences of programs are assessed by their probability in the learned model while low probability ones are marked as bugs. The assumption is that low probability token sequences are unusual, which may indicate bugs, bad practices or special uses of code. 

 
For bug detection the four main components that need to be configured for the model are: \textit{Gram Size, Sequence Length, Reporting Size and Minimum Token Occurrence}. After adjusting these settings, the n-gram Markov model is able to obtain the probabilities of all token sequences. Token sequences in \scratch{} consist of blocks that can be arranged by the user with drag-and-drop. The probability of each block in a sequence is only determined by its previous n-1 tokens. Using a 3-gram model the probability of the sequence s is:
\(P(s) = P(b_{1})P(b_{2}\mid b_{1})P(b_{3}\mid b_{1}b_{2})P(b_{4}\mid b_{2}b_{3}) \). Then the language model ranks the outcome based on the probabilities in descending order and reports the sequences with the lowest probabilities as potential bugs. 